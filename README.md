
md
<p align="center">
  <img src="assets/logo.png" alt="DocuMind Logo" width="170">
</p>

<h1 align="center">ğŸ§  DocuMind â€“ RAG-Based Local Document Assistant</h1>

<p align="center">
A lightweight, private, local Retrieval-Augmented Generation chatbot that answers questions from your PDF and TXT documents â€” powered by FAISS, LangChain, and Ollama.
</p>

---

## ğŸš€ Features

- ğŸ“„ Upload PDFs or TXT files  
- ğŸ” Intelligent retrieval using **FAISS**  
- ğŸ§  Embedding powered by **Ollama (nomic-embed-text)** â€“ No PyTorch required  
- ğŸ¤– LLM responses with **Ollama llama3.2**  
- âš¡ Local, fast, and private  
- ğŸ–¥ Clean **Streamlit UI**  
- ğŸ³ Fully containerized using **Docker**  

---

## ğŸ“‚ Project Structure



RAG_Chatbot/
â”œâ”€â”€ app.py                     # Main Streamlit UI
â”œâ”€â”€ module/
â”‚   â”œâ”€â”€ document_processor.py  # Document loading, splitting, embeddings
â”‚   â”œâ”€â”€ retriever.py           # Semantic retrieval + re-ranking
â”‚   â”œâ”€â”€ generator.py           # Answer generation with Ollama
â”‚   â”œâ”€â”€ utilities.py           # Helper utilities
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ logo.png               # Your DocuMind logo
â”œâ”€â”€ Dockerfile                 # Docker configuration
â”œâ”€â”€ requirements.txt           # Lightweight dependency list
â””â”€â”€ README.md                  # Project documentation

`

---

## ğŸ›  Prerequisites

You must install:

- **Docker** â†’ https://www.docker.com/get-started  
- **Ollama** â†’ https://ollama.ai  

After installing Ollama, pull the required models:

bash
ollama pull nomic-embed-text
ollama pull llama3.2
`

---

## ğŸ³ Running DocuMind With a Single Command

From inside the project folder:

### ğŸ”¨ Build

bash
docker build -t documind .


### â–¶ Run the app

bash
docker run -p 8501:8501 documind


Open your browser at:


http://localhost:8501


---

## ğŸ“ How It Works

### 1ï¸âƒ£ Document Upload

The user uploads one or multiple **PDF/TXT** files.

### 2ï¸âƒ£ Preprocessing

Documents are:

* Loaded using PyPDFLoader / TextLoader
* Cleaned & validated
* Split into chunks with RecursiveCharacterTextSplitter

### 3ï¸âƒ£ Embedding (Ollama)

Text chunks â†’ **OllamaEmbeddings ("nomic-embed-text")**
âœ” No PyTorch
âœ” No Transformers
âœ” Low memory + fast

### 4ï¸âƒ£ Indexing

Chunks stored inside **FAISS** (local semantic vector search).

### 5ï¸âƒ£ Retrieval

Best chunks are selected using:

* base_retriever (FAISS)
* contextual re-ranking (LLMChainExtractor + llama3.2)

### 6ï¸âƒ£ Answer Generation

The final context is fed into **llama3.2**:

You get a grounded, non-hallucinated answer.

---

## ğŸ§ª Example Prompt

Ask something like:


What does this document say about neural networks?


DocuMind will fetch relevant chunks and generate an answer.

---

## ğŸ”§ Technologies Used

* **LangChain** â€“ Retrieval + processing
* **FAISS** â€“ Fast vector search
* **Ollama** â€“ Local LLM + Embeddings
* **Streamlit** â€“ User interface
* **Docker** â€“ Containerized deployment

---

## ğŸ¤ Contributors

### ğŸ›  Developers

* **Suhani Verma** â€“ RAG pipeline, embeddings, retrieval, LLM generation
* **Arya Jha** â€“ Interface design & Docker workflow
* **Ria Kumari** â€“ Documentation & repository structure

---

## ğŸ“œ License

This project is licensed under the **MIT License**.

---

## â­ If you like this projectâ€¦

Consider giving the repo a **star** â­ on GitHub!


gitHub.com/TheSuhaniVerma/RAG_Chatbot
```

